{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is used to predict completly new molecules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No normalization for SPS. Feature removed!\n",
      "No normalization for AvgIpc. Feature removed!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Unable to import pysam. Please make sure it is installed.\n",
      "Error: Unable to import pysam. Please make sure it is installed.\n",
      "Error: Unable to import pysam. Please make sure it is installed.\n",
      "WARNING:tensorflow:From c:\\Users\\wanne\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\wanne\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:588: calling function (from tensorflow.python.eager.polymorphic_function.polymorphic_function) with experimental_relax_shapes is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "experimental_relax_shapes is deprecated, use reduce_retracing instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. No module named 'dgl'\n",
      "Skipped loading modules with transformers dependency. No module named 'transformers'\n",
      "cannot import name 'HuggingFaceModel' from 'deepchem.models.torch_models' (c:\\Users\\wanne\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\deepchem\\models\\torch_models\\__init__.py)\n",
      "Skipped loading modules with pytorch-lightning dependency, missing a dependency. No module named 'lightning'\n",
      "Skipped loading some Jax models, missing a dependency. No module named 'jax'\n"
     ]
    }
   ],
   "source": [
    "import deepchem as dc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.utils._testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF,RationalQuadratic, Matern, WhiteKernel, ConstantKernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "prediciting: TB\n",
      "\n",
      "loading the TB data...\n",
      "train-r2: 1.0 valid-r2: 0.998\n",
      "CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC TB :  544.122848033905\n",
      "CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC TB :  610.8878096938133\n",
      "CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC TB :  537.4652728438377\n",
      "CCCCO TB :  58.943196415901184\n"
     ]
    }
   ],
   "source": [
    "@ignore_warnings(category=ConvergenceWarning)\n",
    "def main():\n",
    "    #load the dataset (polymer smiles and a specific task)\n",
    "\n",
    "    #tasks = [\"PC\", \"TB\", \"TC\"]\n",
    "    tasks = [\"TB\"]\n",
    "    # read the csv file containening the neccesarry rdkit descriptors\n",
    "    feats_df = pd.read_csv('../csv_files/RFE_features.csv')\n",
    "\n",
    "\n",
    "    for task in tasks:\n",
    "\n",
    "        # read the dataset belonging to the rigth property\n",
    "        print(\"\\n\\nprediciting:\",task)\n",
    "        file = '../csv_files/Combined_' + task + '.csv'\n",
    "\n",
    "        #get the descriptors neccesarry for the property\n",
    "        descriptors = feats_df[task]\n",
    "        descriptorlist = descriptors.dropna().to_list()\n",
    "\n",
    "        print(\"\\nloading the\", task, \"data...\")\n",
    "        loader = dc.data.CSVLoader([task], feature_field=\"smiles\", featurizer=dc.feat.RDKitDescriptors(descriptorlist))\n",
    "        Data = loader.create_dataset(file)\n",
    "\n",
    "        #some RDKit descriptors return nan, make these 0\n",
    "        X = np.nan_to_num(Data.X, copy=True, nan=0.0, posinf=0)\n",
    "\n",
    "        #add data to dataset with the according propertie to be predicted\n",
    "        Dataset = dc.data.DiskDataset.from_numpy(X=X, y=Data.y, w=Data.w, ids=Data.ids, tasks = [task])\n",
    "        \n",
    "    \n",
    "            \n",
    "        #split the dataset using the random splitter\n",
    "        splitter = dc.splits.RandomSplitter()\n",
    "        train_dataset, valid_dataset = splitter.train_test_split(Dataset, frac_train = 0.99)\n",
    "\n",
    "\n",
    "        # create the GPR model and fit the model\n",
    "        kernel = 1 * RationalQuadratic() + 1 * WhiteKernel() + 1 * ConstantKernel() * RationalQuadratic()\n",
    "        model = dc.models.SklearnModel(GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10))\n",
    "        model.fit(train_dataset)\n",
    "        \n",
    "        #calculate r2 scores\n",
    "        metric = dc.metrics.Metric(dc.metrics.pearson_r2_score)\n",
    "        train_r2score = model.evaluate(train_dataset, metric)\n",
    "        valid_r2score = model.evaluate(valid_dataset, metric)\n",
    "\n",
    "        #make then useable and print them\n",
    "        validr2=list(valid_r2score.values())[0]\n",
    "        trainr2=list(train_r2score.values())[0]\n",
    "        print(\"train-r2:\",round(trainr2,3),\"valid-r2:\", round(validr2,3))\n",
    "\n",
    "        #load the test set into a list\n",
    "        SmileList = pd.read_csv('../csv_files/test_data.csv')[\"smiles\"].tolist()\n",
    "\n",
    "        #featurize the test set\n",
    "        featurizer = dc.feat.RDKitDescriptors(descriptorlist)\n",
    "        features = featurizer.featurize(SmileList)\n",
    "        X_topredict = np.nan_to_num(features, copy=True, nan=0.0)\n",
    "\n",
    "        #add it to a deepchem datastructure\n",
    "        test_dataset = dc.data.DiskDataset.from_numpy(X=X_topredict, ids=SmileList, tasks = [\"Eat\"])\n",
    "\n",
    "        #predict the test set\n",
    "        predicted_test = model.predict(test_dataset)\n",
    "\n",
    "        for idx, smiles in enumerate(SmileList):\n",
    "            print(smiles,task, \": \",predicted_test[idx])\n",
    "\n",
    "main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
