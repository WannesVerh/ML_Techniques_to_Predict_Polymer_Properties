{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we saw good results concerning the prediction of atomization energy, we will now tried to predict the atomization energy of the polyols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No normalization for SPS. Feature removed!\n",
      "No normalization for AvgIpc. Feature removed!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Unable to import pysam. Please make sure it is installed.\n",
      "Error: Unable to import pysam. Please make sure it is installed.\n",
      "Error: Unable to import pysam. Please make sure it is installed.\n",
      "WARNING:tensorflow:From c:\\Users\\wanne\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\wanne\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:588: calling function (from tensorflow.python.eager.polymorphic_function.polymorphic_function) with experimental_relax_shapes is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "experimental_relax_shapes is deprecated, use reduce_retracing instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. No module named 'torch_geometric'\n",
      "Skipped loading modules with transformers dependency. No module named 'transformers'\n",
      "cannot import name 'HuggingFaceModel' from 'deepchem.models.torch_models' (c:\\Users\\wanne\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\deepchem\\models\\torch_models\\__init__.py)\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. cannot import name 'DMPNN' from 'deepchem.models.torch_models' (c:\\Users\\wanne\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\deepchem\\models\\torch_models\\__init__.py)\n",
      "Skipped loading modules with pytorch-lightning dependency, missing a dependency. No module named 'lightning'\n",
      "Skipped loading some Jax models, missing a dependency. No module named 'jax'\n"
     ]
    }
   ],
   "source": [
    "# Train a GPR model with RDkit featurization\n",
    "import deepchem as dc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF,RationalQuadratic, Matern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the descriptors that will be used to describe the polymers\n",
    "file = open('../TrainingTheNN/feat_selection/feats.txt', 'r')\n",
    "content = file.read()\n",
    "descriptorlist = content.strip('\\n').split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the data...\n"
     ]
    }
   ],
   "source": [
    "#load the dataset (polymer smiles and their energy of atomization)\n",
    "\n",
    "print(\"loading the data...\")\n",
    "loader1 = dc.data.CSVLoader([\"Eat\"], feature_field=\"smiles\", featurizer=dc.feat.RDKitDescriptors(descriptorlist))\n",
    "Data1 = loader1.create_dataset('../csv_files/Polymers_Eat.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some RDKit descriptors return nan, make these 0\n",
    "X1 = np.nan_to_num(Data1.X, copy=True, nan=0.0)\n",
    "#add data to dataset\n",
    "Dataset = dc.data.DiskDataset.from_numpy(X=X1, y=Data1.y, w=Data1.w, ids=Data1.ids, tasks = [\"Eat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wanne\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:663: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  _check_optimize_result(\"lbfgs\", opt_res)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model is fitted\n",
      "Training set r2_score: {'pearson_r2_score': 0.9999999999999973}\n",
      "Test set r2_score: 0.964\n",
      "tets set RMSE_score: 0.066\n"
     ]
    }
   ],
   "source": [
    "#split the dataset using the random splitter\n",
    "splitter = dc.splits.RandomSplitter()\n",
    "train_dataset, test_dataset = splitter.train_test_split(Dataset)\n",
    "\n",
    "# create the GPR model & fit the model\n",
    "kernel = 1 * RationalQuadratic()\n",
    "model = dc.models.SklearnModel(GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=20))\n",
    "\n",
    "print(\"fitting model...\")\n",
    "model.fit(train_dataset)\n",
    "print(\"model is fitted\")\n",
    "\n",
    "#predict the test set\n",
    "predicted = model.predict(test_dataset)\n",
    "\n",
    "#calculate r2 score\n",
    "metric = dc.metrics.Metric(dc.metrics.pearson_r2_score)\n",
    "print('Training set r2_score:', model.evaluate(train_dataset, metric))\n",
    "test_score= model.evaluate(test_dataset, metric)\n",
    "print('Test set r2_score:',round(list(test_score.values())[0],3))\n",
    "\n",
    "#calculate RMSE score\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "RMSE_score = root_mean_squared_error(test_dataset.y,predicted)\n",
    "print('tets set RMSE_score:',round(RMSE_score,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above the model was trained and checked, now its time to predict unknown polymers like our glycols!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will predict polyglycols with a mass from 238g/mol -987g/mol\n",
    "SmileList = [\"[*]CCO[*]\",\n",
    "\"C(COCCOCCOCCOCCOCCOCCOCCO)O\",\n",
    "\"C(COCCOCCOCCOCCOCCOCCOCCOCCOCCOCCO)O\",\n",
    "\"C(COCCOCCOCCOCCOCCOCCOCCOCCOCCOCCOCCOCCOCCO)O\",\n",
    "\"C(COCCOCCOCCOCCOCCOCCOCCOCCOCCOCCOCCOCCOCCOCCOCCOCCO)O\",\n",
    "\"C(COCCOCCOCCOCCOCCOCCOCCOCCOCCOCCOCCOCCOCCOCCOCCOCCOCCOCCOCCO)O\",\n",
    "\"C(COCCOCCOCCOCCOCCOCCOCCOCCOCCOCCOCCOCCOCCOCCOCCOCCOCCOCCOCCOCCOCCO)O\",\n",
    "\"C[C@@H](OC[C@@]1(c2ccccc2)CC[C@]2(CCC(=O)N2)CN1)c1cc(C(F)(F)F)cc(C(F)(F)F)c1\",\n",
    "\"CCC(Cc1c(I)cc(I)c(N)c1I)C(=O)O\",\n",
    "\"CN(C(=O)CC(=O)N(C)c1c(I)c(C(=O)NC(CO)C(O)CO)c(I)c(C(=O)NC(CO)C(O)CO)c1I)c1c(I)c(C(=O)NC(CO)C(O)CO)c(I)c(C(=O)NC(CO)C(O)CO)c1I\",\n",
    "\"CC[C@H](C)C(=O)O[C@H]1C[C@H](O)C=C2C=C[C@H](C)[C@H](CC[C@@H](O)C[C@@H](O)CC(=O)O)[C@H]21\",\n",
    "\"CC(=O)c1ccc(S(=O)(=O)NC(=O)NC2CCCCC2)cc1\",\n",
    "\"CC(C)c1cccc(C(C)C)c1O\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#featurize these polymers\n",
    "featurizer = dc.feat.RDKitDescriptors(descriptorlist)\n",
    "features = featurizer.featurize(SmileList)\n",
    "X_topredict = np.nan_to_num(features, copy=True, nan=0.0)\n",
    "#add it to a deepchem datastructure\n",
    "PredicDataset = dc.data.DiskDataset.from_numpy(X=X_topredict, ids=SmileList, tasks = [\"Eat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*]CCO[*] atomization energy: -5.34921144056716\n",
      "C(COCCOCCOCCOCCOCCOCCOCCO)O atomization energy: -5.324097814853303\n",
      "C(COCCOCCOCCOCCOCCOCCOCCOCCOCCOCCO)O atomization energy: -5.3071331044484396\n",
      "C(COCCOCCOCCOCCOCCOCCOCCOCCOCCOCCOCCOCCOCCO)O atomization energy: -5.355355315114139\n",
      "C(COCCOCCOCCOCCOCCOCCOCCOCCOCCOCCOCCOCCOCCOCCOCCOCCO)O atomization energy: -5.39991001454473\n",
      "C(COCCOCCOCCOCCOCCOCCOCCOCCOCCOCCOCCOCCOCCOCCOCCOCCOCCOCCOCCO)O atomization energy: -5.421975081873825\n",
      "C(COCCOCCOCCOCCOCCOCCOCCOCCOCCOCCOCCOCCOCCOCCOCCOCCOCCOCCOCCOCCOCCO)O atomization energy: -5.425174361851532\n",
      "C[C@@H](OC[C@@]1(c2ccccc2)CC[C@]2(CCC(=O)N2)CN1)c1cc(C(F)(F)F)cc(C(F)(F)F)c1 atomization energy: -5.912036958819954\n",
      "CCC(Cc1c(I)cc(I)c(N)c1I)C(=O)O atomization energy: -5.323301372322021\n",
      "CN(C(=O)CC(=O)N(C)c1c(I)c(C(=O)NC(CO)C(O)CO)c(I)c(C(=O)NC(CO)C(O)CO)c1I)c1c(I)c(C(=O)NC(CO)C(O)CO)c(I)c(C(=O)NC(CO)C(O)CO)c1I atomization energy: -5.418012212991016\n",
      "CC[C@H](C)C(=O)O[C@H]1C[C@H](O)C=C2C=C[C@H](C)[C@H](CC[C@@H](O)C[C@@H](O)CC(=O)O)[C@H]21 atomization energy: -5.51568624349602\n",
      "CC(=O)c1ccc(S(=O)(=O)NC(=O)NC2CCCCC2)cc1 atomization energy: -5.798334389590309\n",
      "CC(C)c1cccc(C(C)C)c1O atomization energy: -5.423005550765083\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(PredicDataset)\n",
    "for idx, smiles in enumerate(SmileList):\n",
    "    print(smiles,\"atomization energy:\",predictions[idx])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
